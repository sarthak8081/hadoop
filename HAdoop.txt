1) ADD NEW USER WITH NAME HDUSER FOR HADOOP OPERATIONS

sudo addgroup hadoop

sudo adduser --ingroup hadoop hduser

sudo adduser hduser sudo

#log in to hduser

2)INSTALL JAVA

sudo apt-get update


#Download jdk-8u151-linux-x64.tar.gz
#Download jre-8u151-linux-x64.tar.gz
#Download hadoop-2.6.5.tar.gz

sudo mkdir /usr/local/java

cd Downloads/


sudo cp -r jdk-8u151-linux-x64.tar.gz /usr/local/java
sudo cp -r jre-8u151-linux-x64.tar.gz /usr/local/java



cd /usr/local/java


sudo tar xvzf jdk-8u151-linux-x64.tar.gz
sudo tar xvzf jre-8u151-linux-x64.tar.gz 

cd

sudo gedit /etc/profile


#Paste at the end and save

#
JAVA_HOME=/usr/local/java/jdk1.8.0_151
PATH=$PATH:$HOME/bin:$JAVA_HOME/bin
JRE_HOME=/usr/local/java/jre1.8.0_151
PATH=$PATH:$HOME/bin:$JRE_HOME/bin
export JAVA_HOME
export JRE_HOME
export PATH
#



3) DISABLE IPV6



sudo gedit /etc/sysctl.conf

#Paste at the end and save

#
#disable ipv6
net.ipv6.conf.all.disable_ipv6 = 1
net.ipv6.conf.default.disable_ipv6 = 1
net.ipv6.conf.lo.disable_ipv6 = 1
#


sudo reboot

which java

which javac

java -version

javac -version


4) INSTALL SSH


sudo apt-get install ssh

y


ssh-keygen -t rsa -P ""

cat $HOME/.ssh/id_rsa.pub >> $HOME/.ssh/authorized_keys

ssh localhost

yes

exit


5) INSTALL HADOOP


cd Downloads/


sudo mv hadoop-2.6.5.tar.gz /usr/local/

cd /usr/local/

sudo tar xvzf hadoop-2.6.5.tar.gz

sudo mv hadoop-2.6.5 hadoop

sudo chown -R hduser:hadoop /usr/local/hadoop

cd


7) HADOOP CONFIGURATION

7.1) Configure Path for JAVA and HADOOP

sudo gedit .bashrc


#paste at the end and save

###
#HADOOP VARIABLES START
export JAVA_HOME=/usr/local/java/jdk1.8.0_151
export HADOOP_INSTALL=/usr/local/hadoop
export PATH=$PATH:$HADOOP_INSTALL/bin
export PATH=$PATH:$HADOOP_INSTALL/sbin
export HADOOP_MAPRED_HOME=$HADOOP_INSTALL
export HADOOP_COMMON_HOME=$HADOOP_INSTALL
export HADOOP_HDFS_HOME=$HADOOP_INSTALL
export YARN_HOME=$HADOOP_INSTALL
export HADOOP_COMMON_LIB_NATIVE_DIR=$HADOOP_INSTALL/lib/native
export HADOOP_OPTS="-Djava.library.path=$HADOOP_INSTALL/lib"
#HADOOP VARIABLES END

#### 


source ~/.bashrc



readlink -f /usr/bin/javac 


7.2) Configure ENV for HADOOP

cd /usr/local/hadoop/etc/hadoop/


sudo gedit hadoop-env.sh

#PASTE at the TOP and save

#
export JAVA_HOME=/usr/local/java/jdk1.8.0_151
#




7.3)Configure core-site.xml


sudo gedit core-site.xml

 
#

<configuration>
<property>
  <name>hadoop.tmp.dir</name>
  <value>/app/hadoop/tmp</value>
</property>

<property>
 <name>fs.default.name</name>
 <value>hdfs://localhost:54310</value>
</property>

</configuration>

#

7.4)Configure hdfs-site.xml


sudo gedit hdfs-site.xml

#

<configuration>
    <property>
        <name>dfs.replication</name>
        <value>1</value>
    </property>
	<property>
		<name>dfs.namenode.name.dir</name>
		<value>file:///usr/local/hadoop_store/hdfs/namenode</value>
	</property>
	<property>
		<name>dfs.datanode.data.dir</name>
		<value>file:///usr/local/hadoop_store/hdfs/datanode</value>
	</property>
</configuration>

#

7.5)Configure mapred-site.xml


cp mapred-site.xml.template mapred-site.xml

sudo gedit mapred-site.xml

#

<configuration>
    <property>
        <name>mapreduce.framework.name</name>
        <value>yarn</value>
    </property>
</configuration>

#

7.6)Configure yarn-site.xml


sudo gedit yarn-site.xml


#

<configuration>
    <property>
        <name>yarn.nodemanager.aux-services</name>
        <value>mapreduce_shuffle</value>
    </property>
</configuration>


#

7.6) Create hdfs DIR for namenode and datanode

cd

sudo mkdir -p /app/hadoop/tmp

sudo mkdir -p /usr/local/hadoop_store/hdfs/namenode

sudo mkdir -p /usr/local/hadoop_store/hdfs/datanode

sudo chown -R hduser:hadoop /usr/local/hadoop_store

sudo chown -R hduser:hadoop /app/hadoop/tmp


7.7) format namenode

 

hadoop namenode -format


8) START HADOOP CLUSTER

start-dfs.sh 

yes 

start-yarn.sh



8.1) verify hadoop cluster started or not

jps

#open below addresses in browser window

#namonode information
localhost:50070

#datanode information
localhost:50090


#resource manager
localhost:8088

stop-dfs.sh 

stop-yarn.sh


/usr/local/hadoop/sbin/mr-jobhistory-daemon.sh start historyserver

/usr/local/hadoop/sbin/mr-jobhistory-daemon.sh stop historyserver








